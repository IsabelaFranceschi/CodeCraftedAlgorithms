{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpY_imebkUtS"
   },
   "source": [
    "# Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0Wj4VOskUtV"
   },
   "source": [
    "A decision tree is a method for decision-making that follows a structured path of choices. This algorithm operates by recursively dividing the dataset into smaller subsets based on the most informative attributes at each step. To put it simply, it uses attributes as decision nodes, and each node divides the data based on the target labels. The order in which attributes are selected can be determined by assessing the reduction in impurity, as measured by metrics like *information gain*, which identifies the most informative attribute for decision-making. The tree keeps dividing the data based on these attributes until a stopping condition is met. These conditions are not limited to but may include when all data belongs to a single target class, there are no more attributes to split on, impurity measures meet predefined thresholds, a maximum depth is reached, or the data cannot be further divided into pure subsets. Once one of these conditions is met, the node becomes a leaf, and the decision is defined by the target class.\n",
    "\n",
    "In short, decisions are made based on conditions related to attributes. Internal nodes represent these conditions, while leaf nodes are the final decision based on those conditions. The root node is the starting attribute of the tree, with subsequent attributes represented as internal nodes, and each leaf node corresponds to a specific class label. See an example in the following image sourced from datacamp [[1]](https://www.datacamp.com/tutorial/decision-tree-classification-python), which predicts the risk of a heart attack based on an individual's health conditions.\n",
    "\n",
    "<img src=\"datacamp.jpeg\" alt=\"title\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mrcR-78kUtZ"
   },
   "source": [
    "### Iterative Tree Building Process in 4 steps (ID3-based):\n",
    "\n",
    "This code builds a decision tree from scratch based on the ID3 concept, a decision tree algorithm introduced by Ross Quinlan. ID3 means Iterative Dichotomiser 3, name derived from its iterative approach, where attributes are iteratively divided (dichotomized) into two or more groups at each step. This algorithm begins at the top and, in each iteration, chooses the best attribute to create a node. In this code, the tree-building process will follow these 4 steps:\n",
    "\n",
    "**1. Attribute Selection:**\n",
    "\n",
    "Identify the most informative attribute in the dataset.\n",
    "\n",
    "**2. Tree Node Creation and Dataset Update:**\n",
    "\n",
    "Create a tree node using the selected attribute, with its distinct attribute values as branches. If a pure class is reached, add a leaf node to to represent the decision. If the node is impure, introduce an expandable node (marked as '###') within the tree node. Update the dataset by removing instances of the pure class when applicable.\n",
    " \n",
    "**3. Tree Expansion:**\n",
    "\n",
    "Continue expanding the branch associated with the next impure node ('###') using the updated dataset. Continue this process iteratively and stop it when no more information gain can be achieved.\n",
    "\n",
    "**4 Repetition:**\n",
    "\n",
    "Repeat this iterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuOKUDiqkUta"
   },
   "source": [
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Decision Tree from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex5o_g_MkUtb"
   },
   "source": [
    "This code uses two short datasets to make it easier to see how the tree is created. Then, it applies the created decision tree to a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "_wg0dUarkUtd"
   },
   "outputs": [],
   "source": [
    "# Libraries needed to generate the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data1 - the target is the variable 'Disease'\n",
    "data1 = pd.DataFrame({'High_bloodpressure': [\"yes\", \"yes\",\"yes\",\"yes\",\"yes\",\"no\",\"no\",\"no\",\"yes\",\"no\",\"no\",\"no\"], \n",
    "                     'Education': [\"high school\", \"high school\",\"college\",\"college\",\n",
    "                                   \"collage or above\",\"high school\",\"9th-11th grade\",\"collage or above\",\n",
    "                                   \"collage or above\",\"less than 9th grade\",\"collage or above\",\"college\"],\n",
    "                    'Disease': [\"yes\", \"yes\",\"yes\",\"no\",\"no\",\"no\",\"no\",\"no\",\"yes\",\"no\",\"no\",\"no\"]})\n",
    "\n",
    "# data2 - the target is the variable 'Class'\n",
    "data2 = pd.DataFrame()\n",
    "data2['Outlook'] = ['sunny', 'sunny', 'overcast', 'rain', 'rain'\n",
    "    , 'rain', 'overcast', 'sunny', 'sunny', 'rain'\n",
    "    , 'sunny', 'overcast', 'overcast', 'rain']\n",
    "data2['Temperature'] = ['hot', 'hot', 'hot', 'mild', 'cool'\n",
    "    , 'cool', 'cool', 'mild', 'cool', 'mild'\n",
    "    , 'mild', 'mild', 'hot', 'mild',]\n",
    "data2['Humidity'] = ['high', 'high', 'high', 'high', 'normal'\n",
    "    , 'normal', 'normal', 'high', 'normal', 'normal'\n",
    "    , 'normal', 'high', 'normal', 'high']\n",
    "data2['Windy'] = ['false', 'true', 'false', 'false', 'false'\n",
    "    , 'true', 'true', 'false', 'false', 'false'\n",
    "    , 'true', 'true', 'false', 'true']\n",
    "data2['Class'] = ['N', 'N', 'P', 'P', 'P'\n",
    "    , 'N', 'P', 'N', 'P', 'P'\n",
    "    , 'P', 'P', 'P', 'N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glk06kjWkUtg"
   },
   "source": [
    "### 1. Attribute Selection:\n",
    "*Identify the most informative attribute in the dataset.*\n",
    "\n",
    "One approach to define the most informative attribute is to select the one that exhibits the highest information gain when splitting the data. This is determined by measuring the impurity of the split. In this code, entropy is applied as measure of impurity. **Entropy** indicates the disorder of the splitting regarding the target class. To find the best split, it is selected the attribute with the lowest entropy, as it means a more informative separation. The entropy formula for a decision tree is defined as [[2]](https://pdf.sciencedirectassets.com/280203/1-s2.0-S1877050922X00100/1-s2.0-S1877050922011905/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCybY5KNXgE9x3WawiSiPp4OiVvt6TPcnr%2F9OBZZXVH2QIhAKwmOP3d1xCvlp%2Fndo692S6XLmlk2vvIJsdNPR3T3L2eKrMFCBoQBRoMMDU5MDAzNTQ2ODY1IgyOJ%2FyKFXl7Ij%2FDL8oqkAVLNKynapBeb55Ow1AhX0kJJj79U%2BNG15BhCifpdH8K9DmSl0ODdDUo4b%2BG%2FOczZCmF8PKlMW4zhlw%2FDMQ5UB8jwu%2BvJWF1Qr2kjktQDJPP8Eyy%2B6pG8vzBDAbwfipian2gNKujhv1FpRSv4eY7Aia7B5TEi58DvfPuBrnRscmDXvIA4iViY6tysAtAHAWIakYh5VvlFOnUs9HuRzHOz%2BguxztcziU6wCCqatBAqulZB2Ko1m%2BQJfORjbiJRtTlYLTtgnDv6yOHZULz6FQ92eHObAif7UjRm43YMFvWk6DJ7ax0FRYw%2BTvb1vFJ7wQaOMeSONYf3tfZcV%2FtNEiIocImqoknFDPahxdQjn0ynKjag9wm7XNWsFh5BJ5EXRzmlreN%2FFlGlOg2oMNxycgDUbYA6e3Qm617jZCAF%2BpeKI1yBycDKfSaLm%2BQXJSob5fPDKyesr3PkY6R2C%2FzUNYkiFH9uZYqMfx3QFlXepwBQbaDnnbQTKMROOS4EkdLThfK2QseFTj54txIxEex4njhHbuvat6CqY15Q9FZsR2SOEj5cfuhXzdaQTg9S9oYsSVpk73ECDrufw19t8akzukzL7eH0TrOynOBeUMCIQqsoDYiSIT05kwPlVf1uz7V7PcbqaSMrnEgB412B40za6OOpK%2F4MD11tzxY8oEEVgOW6HuZuFccx9OIZ44Mq1ztIyp9AVJ1DFqOSEm6J7zM8km3ALARBQagnnUIvgkwpYFbHOJzq%2BuQJ7BXlYNfeRtVkXRw%2Bj44QqPA4lwI8F4dgVmqnAK%2FI%2BgCTmaEn15%2BoxElLkBdlnWWiQkXAUA%2B%2FX7ybcXETMXyxmZc21d%2BO%2Ff6PfFchRqAHEe6p5mDcjLS74%2Bl71VHVDCt%2FomqBjqwAexnM7snaaaPLinat86AwtwLbm9NYbfG5dMddjgyZR3lixv7CMEwZBCocgMcoo1x3xWeTTmU3nDvmTbm3MXOUWMewPlK9T1q8H6Y%2BI8rqOv6q7ZOe5Bdr7SSgsOv1RBDh2RHHIHlDLIQkx1vexGywrF3jO5LMznaB%2BvEIy1tUQECsLkok25ObxcNpmoWXvq0ZkZXVlxSTN0TckLAJ0sVIsv938qqpKYQzz4aTKWrmhoK&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20231101T165008Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYTMAISKDL%2F20231101%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=951dabe3bb8ef44d7b75cffddd87e35ab076d0029a67b63dd3fca88e9c5b7258&hash=33293e407626590875cb056f4ce39e1ff01996fbde84d02722d4f32989a553b0&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1877050922011905&tid=spdf-0b9c55f4-5569-4264-b708-0f06dac3bd05&sid=cee107b52c557344c638e7e39807b01f6d9fgxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0217575607550e545c&rr=81f5a28f1edc3264&cc=at):\n",
    "\n",
    "\\begin{gather*}\n",
    "E = - \\sum \\limits _{i=1} ^{k} p_i*log_2(p_i)\\\\\n",
    "\\end{gather*}\n",
    "\n",
    "Where:\n",
    "- E is the entropy of the dataset,\n",
    "- k is the number of distinct classes in the dataset,\n",
    "- $p_{i}$ the proportion of the observations belonging to class i.\n",
    "\n",
    "Then, the **Information Gain** is calculated by comparing the entropy of the dataset before and after a specific transformation. In other words, information gain quantifies the effectiveness of a split by considering the entropy of each branch and weighting it according to the number of elements it contains. \n",
    "\n",
    "So, to choose the decision nodes, the following steps will be done:\n",
    "\n",
    "a) Compute the entropy of the entire dataset;\n",
    "\n",
    "b) Compute the information gain for each individual attribute;\n",
    "\n",
    "c) Identify the most informative attribute by selecting the one with the highest information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OX1s2uVkUth"
   },
   "source": [
    "#### a) Compute the entropy of the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HNYTJe5BkUti"
   },
   "outputs": [],
   "source": [
    "# Function to caclculate the entropy of the dataset. Note that it is calculated based on the target variable.\n",
    "def total_entropy (df, target):\n",
    "    # Get the total number of observations in df\n",
    "    total_observations = df.shape[0]\n",
    "    # Initialize the total entropy to zero\n",
    "    total_entropy = 0\n",
    "    \n",
    "    # Loop through each unique label in the target column of df\n",
    "    for label in df[target].unique(): \n",
    "        # Count how many times the current label appears in the 'target' column\n",
    "        label_count = df[df[target] == label].shape[0] \n",
    "        # Calculate the proportion of observations with the current label\n",
    "        label_proportion = label_count/total_observations\n",
    "        # Calculate the entropy of the label using the formula for entropy\n",
    "        label_entropy = -label_proportion*np.log2(label_proportion)\n",
    "        # Add the label's entropy to the total entropy of df\n",
    "        total_entropy += label_entropy\n",
    "        \n",
    "    # Return the total entropy of df    \n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "MMtZ_Ek6kUti",
    "outputId": "ed59ada0-39ca-47d5-923e-5bc7c57a400c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total entropy for data1\n",
    "total_entropy (data1, \"Disease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Yfsy8jDDkUtj",
    "outputId": "3b8c25bd-1372-4087-e4d7-2ba9a09a4307"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9402859586706311"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total entropy for data2\n",
    "total_entropy (data2, \"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAiraklCkUtk"
   },
   "source": [
    "#### b) Compute the information gain for each individual attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nX3YZC2kkUtk"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the information gain considering a single attribute\n",
    "def info_gain(attribute, df, target):\n",
    "    # Get the unique labels of the attribute\n",
    "    attribute_label_list = df[attribute].unique()\n",
    "    # Get the total number of observations in df\n",
    "    total_observations = df.shape[0]\n",
    "    \n",
    "    # Create an empty list to store the entropies\n",
    "    entropies=[]\n",
    "    # Loop through each unique label of the attribute\n",
    "    for attribute_label in attribute_label_list:\n",
    "         # Filter the data for the current attribute label\n",
    "        data = df[df[attribute] == attribute_label]\n",
    "        # Calculate the proportion of observations with the current attribute label\n",
    "        attribute_label_proportion = data.shape[0]/total_observations\n",
    "        \n",
    "        # Initialize the entropy for the current attribute label\n",
    "        entropy = 0\n",
    "        # Loop through each unique label in the target variable\n",
    "        for label in df[target].unique():\n",
    "            # Count how many times the current label appears in the filtered data\n",
    "            label_target_count = data[data[target] == label].shape[0] \n",
    "            # Check if the label appears in the filtered data\n",
    "            if label_target_count != 0:\n",
    "                # Calculate the proportion of the label in the filtered data\n",
    "                proportion_label = data[data[target] == label].shape[0]/data.shape[0]\n",
    "                # Calculate the entropy for the label in the filtered data\n",
    "                entropy_label = -proportion_label*np.log2(proportion_label)\n",
    "                # Add the contribution of the label to the entropy for the current attribute label\n",
    "                entropy += attribute_label_proportion*entropy_label\n",
    "            \n",
    "        # Append the calculated entropy for the attribute label to the list\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    # Calculate the information gain for the attribute\n",
    "    info_gain = total_entropy(df, target) - sum(entropies)\n",
    "            \n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "7_3K61MVkUtk",
    "outputId": "c5cb018a-abca-4ab2-c25e-f109552f56c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18872187554086717"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_gain(\"Education\", data1, \"Disease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "u855qG4FkUtl",
    "outputId": "51b9dab1-29d6-4f2e-cdbb-1aa12536ddd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24674981977443933"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_gain(\"Outlook\", data2, \"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that for data1, selecting the \"Education\" variable yields an information gain of approximately 0.19. Likewise, in data2, choosing the \"Outlook\" variable results in an information gain of approximately 0.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-mR03NpkUtl"
   },
   "source": [
    "#### c) Identify the most informative attribute by selecting the one with the highest information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EYDqOBWgkUtm"
   },
   "outputs": [],
   "source": [
    "# Function that returns the most informative attribute\n",
    "def most_informative_attribute(df, target):\n",
    "    # Create a list of attribute names by removing the 'target' column name\n",
    "    attribute_list = df.columns.drop(target)\n",
    "    \n",
    "    # Initialize variables to keep track of the maximum information gain and the corresponding attribute\n",
    "    max_info_gain = -1 # Initialize to a value that's lower than possible information gains\n",
    "    max_info_attribute = None # Initially, no attribute is selected\n",
    "    \n",
    "    # Loop through each attribute in df\n",
    "    for attribute in attribute_list:\n",
    "        # Calculate the information gain for the current attribute\n",
    "        attribute_info_gain = info_gain(attribute, df, target)\n",
    "        \n",
    "        # Check if the information gain for the current attribute is greater than the maximum seen so far\n",
    "        if max_info_gain < attribute_info_gain: \n",
    "            max_info_gain = attribute_info_gain # Update the maximum information gain\n",
    "            max_info_attribute = attribute # Update the attribute with the maximum information gain\n",
    "            \n",
    "    # Return the most informative attribute (the one with the highest information gain)\n",
    "    return max_info_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "YSizsa_XkUtm",
    "outputId": "92a40d97-3093-42e9-a963-80a623d65bb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'High_bloodpressure'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_informative_attribute(data1, \"Disease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "GFh1nAk2kUtn",
    "outputId": "5c2fea53-178d-47b3-9457-e874ee6779fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Outlook'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_informative_attribute(data2, \"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, in data1, 'High_bloodpressure' stands out as the most informative attribute, while in data2, it is 'Outlook'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "435ruBvhkUtn"
   },
   "source": [
    "### 2. Tree Node Creation and Dataset Update\n",
    "\n",
    "*Create a tree node using the selected attribute, with its distinct attribute values as branches. If a pure class is reached, add a leaf node to to represent the decision. If the node is impure, introduce an expandable node (marked as '###') within the tree node. Update the dataset by removing instances of the pure class when applicable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "y3z81aN4kUto"
   },
   "outputs": [],
   "source": [
    "# Function to generate a subtree\n",
    "def generate_sub_tree(attribute, df, target):\n",
    "    # Count the occurrences of each unique attribute value\n",
    "    attribute_label_count = df[attribute].value_counts(sort=False)\n",
    "    # Initialize an empty dictionary to represent the sub-tree or node\n",
    "    sub_tree = {}\n",
    "    \n",
    "    # Loop through each unique attribute label and its count\n",
    "    for attribute_label, count in attribute_label_count.iteritems():\n",
    "        # Create a dataset containing only rows where the attribute matches the current label\n",
    "        attribute_label_data = df[df[attribute] == attribute_label]\n",
    "        \n",
    "        # Initialize a flag to track if the attribute_label represents a pure label\n",
    "        assigned_to_node = False\n",
    "        # Loop through each unique label in the target variable\n",
    "        for label in df[target].unique(): \n",
    "            # Count how many times the current label appears in the attribute_label_data\n",
    "            label_count = attribute_label_data[attribute_label_data[target] == label].shape[0]\n",
    "            # Check if the label count matches the count of the attribute_label (indicating a pure label)\n",
    "            if label_count == count:\n",
    "                # Add a decision node to the sub-tree since it's a pure label\n",
    "                sub_tree[attribute_label] = label\n",
    "                # Remove rows with the current attribute_label from df because it's pure\n",
    "                df = df[df[attribute] != attribute_label] #removing rows with attribute_label because it is pure (updating the df)\n",
    "                # Update the assigned_to_node flag to indicate that it's a pure label\n",
    "                assigned_to_node = True\n",
    "                \n",
    "        # If it's not a pure label, mark the branch with '###' to extend the node\n",
    "        if not assigned_to_node: \n",
    "            sub_tree[attribute_label] = \"###\" # Should extend the node, so the branch is marked with ###\n",
    "            \n",
    "    # Return the sub-tree and the updated dataset\n",
    "    return sub_tree, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "3TskMXdAkUtq",
    "outputId": "5bdaf9ba-cd34-4217-ee9c-45e789957b44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'yes': '###', 'no': 'no'},\n",
       "   High_bloodpressure         Education Disease\n",
       " 0                yes       high school     yes\n",
       " 1                yes       high school     yes\n",
       " 2                yes           college     yes\n",
       " 3                yes           college      no\n",
       " 4                yes  collage or above      no\n",
       " 8                yes  collage or above     yes)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sub_tree(\"High_bloodpressure\", data1, \"Disease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lo4zbKrBkUtr",
    "outputId": "11ebd351-7022-4a98-8009-2896a75cc9f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sunny': '###', 'overcast': 'P', 'rain': '###'},\n",
       "    Outlook Temperature Humidity  Windy Class\n",
       " 0    sunny         hot     high  false     N\n",
       " 1    sunny         hot     high   true     N\n",
       " 3     rain        mild     high  false     P\n",
       " 4     rain        cool   normal  false     P\n",
       " 5     rain        cool   normal   true     N\n",
       " 7    sunny        mild     high  false     N\n",
       " 8    sunny        cool   normal  false     P\n",
       " 9     rain        mild   normal  false     P\n",
       " 10   sunny        mild   normal   true     P\n",
       " 13    rain        mild     high   true     N)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sub_tree(\"Outlook\", data2, \"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2pjDrEukUts"
   },
   "source": [
    "### 3. Tree Expansion\n",
    "*Continue expanding the branch associated with the next impure class ('###') using the updated dataset. Continue this process iteratively and stop this tree-building process when no more information gain can be achieved (tree pruning).*\n",
    "\n",
    "Now, these methods will be combined into a recursive step-by-step approach. The process will stop when one of the following conditions is met:\n",
    "\n",
    "- There is no more data (the dataset becomes empty after updating);\n",
    "- There are no expandable branches left (all nodes are pure);\n",
    "- There is no further increase in information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HL_1GofVkUts"
   },
   "outputs": [],
   "source": [
    "def make_tree(root, node_to_expand, df, target):\n",
    "    # Check if df is not empty after updating\n",
    "    if df.shape[0] != 0:\n",
    "        # Find the most informative attribute for the current df\n",
    "        max_info_attribute = most_informative_attribute(df, target)\n",
    "        # Generate a sub-tree and update df based on the most informative attribute\n",
    "        sub_tree, df = generate_sub_tree(max_info_attribute, df, target)\n",
    "        next_root = None\n",
    "        \n",
    "        if node_to_expand != None:\n",
    "            # Add a dictionary for the current node to the intermediate node of the tree\n",
    "            root[node_to_expand] = dict()\n",
    "            root[node_to_expand][max_info_attribute] = sub_tree\n",
    "            next_root = root[node_to_expand][max_info_attribute]\n",
    "        else:\n",
    "            # Add a dictionary for the current node to the root of the tree\n",
    "            root[max_info_attribute] = sub_tree\n",
    "            next_root = root[max_info_attribute]\n",
    "            \n",
    "        # Calculate the information gain for the current attribute\n",
    "        IG=info_gain(max_info_attribute, df, target)\n",
    "        # Condition to stop the tree\n",
    "        if IG<=0:\n",
    "            # Make a decision and end the tree\n",
    "            for node, branch in list(next_root.items()):\n",
    "                if branch == \"###\":\n",
    "                    next_root[node] = df[target].mode()[0] # Decision\n",
    "        \n",
    "        # Iterate through the nodes of the tree\n",
    "        for node, branch in list(next_root.items()): #iterating the tree node\n",
    "            # Check if the branch is expandable (marked with '###')\n",
    "            if branch == \"###\":\n",
    "                attribute_value_data = df[df[max_info_attribute] == node] # Use the updated dataset\n",
    "                # Recursively build the tree for the branch\n",
    "                make_tree(next_root, node, attribute_value_data, target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFGZQ7bvkUtt"
   },
   "source": [
    "### 4. Repetition:\n",
    "*Repeat this iterative process.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iilrvLdnkUtt"
   },
   "outputs": [],
   "source": [
    "def id3(df, target):\n",
    "    # Initialize an empty tree that will be updated\n",
    "    tree = {}\n",
    "    # Start building the decision tree by calling the recursive function 'make_tree'\n",
    "    make_tree(tree, None, df, target)\n",
    "    # Return the decision tree\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "iw7Ng6FokUtt",
    "outputId": "7813bdce-c1c9-40f2-e393-50a1a4150a97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'High_bloodpressure': {'yes': 'yes', 'no': 'no'}}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id3(data1, \"Disease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CTGo3lbnkUtt",
    "outputId": "2575c93a-e647-49c3-9295-eebd27890c6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Outlook': {'sunny': {'Humidity': {'high': 'N', 'normal': 'P'}},\n",
       "  'overcast': 'P',\n",
       "  'rain': {'Windy': {'false': 'P', 'true': 'N'}}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id3(data2, \"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEEjJ1qUkUtu"
   },
   "source": [
    "### Predictions from the Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WNVuvv4akUtv"
   },
   "outputs": [],
   "source": [
    "def predict(tree, observation):\n",
    "    # Check if the current node in the tree is a leaf node (decision)\n",
    "    if not isinstance(tree, dict): \n",
    "        return tree # Return the decision value\n",
    "    else:\n",
    "        # Get the name of the first attribute from the dictionary (current node)\n",
    "        root_node = next(iter(tree)) \n",
    "        # Get the value of the attribute in the observation data\n",
    "        attribute_value = observation[root_node] \n",
    "        # Check if the attribute value is present in the current tree node\n",
    "        if attribute_value in tree[root_node]:\n",
    "            # Recursively navigate to the next node in the tree\n",
    "            return predict(tree[root_node][attribute_value], observation)\n",
    "        else:\n",
    "            return None # Return None if the attribute value is not found in the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a decision tree using the 'id3' function (data1)\n",
    "tree1=id3(data1, \"Disease\")\n",
    "# Initialize an empty list to store predictions for each observation\n",
    "predictions1=[]\n",
    "# Loop through each observation in the dataset\n",
    "for i in range(data1.shape[0]):\n",
    "    # Get an observation from the dataset\n",
    "    observation=data1.iloc[i, :]\n",
    "    # Use the decision tree 'tree2' to make a prediction for the current observation\n",
    "    pred=predict(tree1, observation)\n",
    "    # Append the prediction to the list of predictions\n",
    "    predictions1.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "AacMW-W5kUtv"
   },
   "outputs": [],
   "source": [
    "# Build a decision tree for data2\n",
    "tree2=id3(data2, \"Class\")\n",
    "predictions2=[]\n",
    "for i in range(data2.shape[0]):\n",
    "    observation=data2.iloc[i, :]\n",
    "    pred=predict(tree2, observation)\n",
    "    predictions2.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gvfZd2MBkUtv",
    "outputId": "c3c59dc8-2213-40e8-db52-42b4a2ba143c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N', 'N', 'P', 'P', 'P', 'N', 'P', 'N', 'P', 'P', 'P', 'P', 'P', 'N']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8wn4e7okUtw"
   },
   "source": [
    "### Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tree, df, target, target_values):\n",
    "    # Initialize True Positives (TP), True Negatives (TN),\n",
    "    # False Positives (FP), and False Negatives (FN) for each target value\n",
    "    class_counts = {value: {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0} for value in target_values}\n",
    "\n",
    "    # Loop through each row in df to make predictions and evaluate them\n",
    "    for index, row in df.iterrows():\n",
    "        prediction = predict(tree, df.iloc[index]) # Prediction using the 'tree' and the current row\n",
    "        actual = df[target].iloc[index] # Actual value from the 'target' column\n",
    "\n",
    "        for value in target_values:\n",
    "            if prediction == value and actual == value:\n",
    "                # If both prediction and actual match the current target value, increment TP\n",
    "                class_counts[value]['TP'] += 1\n",
    "            elif prediction == value and actual != value:\n",
    "                # If prediction is the current target value but actual is not, increment FP\n",
    "                class_counts[value]['FP'] += 1\n",
    "            elif prediction != value and actual == value:\n",
    "                # If prediction is not the current target value but actual is, increment FN\n",
    "                class_counts[value]['FN'] += 1\n",
    "            else:\n",
    "                # If both prediction and actual are not the current target value, increment TN\n",
    "                class_counts[value]['TN'] += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 Score\n",
    "    results = {}\n",
    "    total_TP = total_TN = total_FP = total_FN = 0\n",
    "\n",
    "    for value in target_values:\n",
    "        TP = class_counts[value]['TP']\n",
    "        FP = class_counts[value]['FP']\n",
    "        FN = class_counts[value]['FN']\n",
    "        TN = class_counts[value]['TN']\n",
    "\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "        total_TP += TP\n",
    "        total_FP += FP\n",
    "        total_FN += FN\n",
    "        total_TN += TN\n",
    "\n",
    "        results[f\"Class {value}\"] = {\n",
    "            \"TP\": TP,\n",
    "            \"TN\": TN,\n",
    "            \"FP\": FP,\n",
    "            \"FN\": FN,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0,\n",
    "            \"Accuracy\": (TP + TN) / (TP + TN + FP + FN)\n",
    "        }\n",
    "\n",
    "    # Calculate overall accuracy.\n",
    "    overall_accuracy = (total_TP + total_TN) / (total_TP + total_TN + total_FP + total_FN)\n",
    "    results[\"Overall Accuracy\"] = overall_accuracy\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Class yes': {'TP': 4,\n",
       "  'TN': 6,\n",
       "  'FP': 2,\n",
       "  'FN': 0,\n",
       "  'Precision': 0.6666666666666666,\n",
       "  'Recall': 1.0,\n",
       "  'F1 Score': 0.8,\n",
       "  'Accuracy': 0.8333333333333334},\n",
       " 'Class no': {'TP': 6,\n",
       "  'TN': 4,\n",
       "  'FP': 0,\n",
       "  'FN': 2,\n",
       "  'Precision': 1.0,\n",
       "  'Recall': 0.75,\n",
       "  'F1 Score': 0.8571428571428571,\n",
       "  'Accuracy': 0.8333333333333334},\n",
       " 'Overall Accuracy': 0.8333333333333334}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree1=id3(data1, \"Disease\")\n",
    "evaluate(tree1, data1, \"Disease\", [\"yes\", \"no\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Class N': {'TP': 5,\n",
       "  'TN': 9,\n",
       "  'FP': 0,\n",
       "  'FN': 0,\n",
       "  'Precision': 1.0,\n",
       "  'Recall': 1.0,\n",
       "  'F1 Score': 1.0,\n",
       "  'Accuracy': 1.0},\n",
       " 'Class P': {'TP': 9,\n",
       "  'TN': 5,\n",
       "  'FP': 0,\n",
       "  'FN': 0,\n",
       "  'Precision': 1.0,\n",
       "  'Recall': 1.0,\n",
       "  'F1 Score': 1.0,\n",
       "  'Accuracy': 1.0},\n",
       " 'Overall Accuracy': 1.0}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2=id3(data2, \"Class\")\n",
    "evaluate(tree2, data2, \"Class\", [\"N\", \"P\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oolgM0o2kUt3"
   },
   "source": [
    "Now that the tree is built, the next step is to apply it to a real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEPqN7NdkUt3"
   },
   "source": [
    "# Testing the algorithm with a real dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_YuPdVdkUt9"
   },
   "source": [
    "The dataset, obtained from Kaggle [[3]](https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial/notebook#8.-Import-dataset-), is related to car attributes, with the target variable being \"Class\". According to [[3]](https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial/notebook#8.-Import-dataset-), the dataset contains the following information:\n",
    "\n",
    "- buying: buying price (vhigh, high, med, low);\n",
    "- maint: price of the maintenance (vhigh, high, med, low);\n",
    "- doors: number of doors (2, 3, 4, 5more);\n",
    "- persons: capacity in terms of persons to carry (2, 4, more);\n",
    "- lug_boot: the size of luggage boot (small, med, big);\n",
    "- safety: estimated safety of the car (low, med, high);\n",
    "- class: car class (unacc, acc, good, vgood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "PZIu6v4VkUt9"
   },
   "outputs": [],
   "source": [
    "df_kagle = pd.read_csv('car_evaluation.csv', header=None)\n",
    "col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
    "df_kagle.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety  class\n",
       "0  vhigh  vhigh     2       2    small    low  unacc\n",
       "1  vhigh  vhigh     2       2    small    med  unacc\n",
       "2  vhigh  vhigh     2       2    small   high  unacc"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kagle.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = [2, 3]\n",
    "df_kagle = df_kagle.drop(df_kagle.columns[columns_to_exclude], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint lug_boot safety  class\n",
       "0  vhigh  vhigh    small    low  unacc\n",
       "1  vhigh  vhigh    small    med  unacc\n",
       "2  vhigh  vhigh    small   high  unacc"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kagle.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1728, 5)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kagle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1728 entries, 0 to 1727\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   buying    1728 non-null   object\n",
      " 1   maint     1728 non-null   object\n",
      " 2   lug_boot  1728 non-null   object\n",
      " 3   safety    1728 non-null   object\n",
      " 4   class     1728 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 67.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_kagle.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buying      0\n",
       "maint       0\n",
       "lug_boot    0\n",
       "safety      0\n",
       "class       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kagle.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unacc    1210\n",
       "acc       384\n",
       "good       69\n",
       "vgood      65\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kagle[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "qGagC0jgkUuC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df_kagle, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "VtGRxA_PkUuD",
    "outputId": "767230d5-f70f-4ee2-ce86-d5a1e3133cb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1209, 5)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "uC-B-0PZkUuE",
    "outputId": "e8b57aa7-0ee3-4492-9b64-48bf6231d496"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(519, 5)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "4yfgRho9kUuE"
   },
   "outputs": [],
   "source": [
    "df_train.reset_index(inplace = True)\n",
    "df_train.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "RxaZUtVIkUuE"
   },
   "outputs": [],
   "source": [
    "df_test.reset_index(inplace = True)\n",
    "df_test.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "r10ARF8akUuE",
    "outputId": "b1b61e29-be46-4a72-c9e5-18602cac4ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 1.02 seconds to build the tree based on train data\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "tree_RealData=id3(df_train, \"class\")\n",
    "end = time.time()\n",
    "print(f\"It took {round((end - start),2)} seconds to build the tree based on train data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "otPgLbuakUuF",
    "outputId": "5adfc1f6-3a7d-4d1a-ab38-5e71f2b34110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with test data: 91.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy_test_=evaluate(tree_, df_test, \"class\")\n",
    "print(f\"Accuracy with test data: {round(accuracy_test_,2)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unacc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'vgood',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'good',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'vgood',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " None,\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " None,\n",
       " 'acc',\n",
       " 'unacc',\n",
       " None,\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " None,\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " None,\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'vgood',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'vgood',\n",
       " 'good',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'vgood',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " None,\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'good',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'vgood',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'good']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_RealData=[]\n",
    "for i in range(df_test.shape[0]):\n",
    "    observation=df_test.iloc[i, :]\n",
    "    pred=predict(tree_RealData, observation)\n",
    "    predictions_RealData.append(pred)\n",
    "predictions_RealData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Class unacc': {'TP': 673,\n",
       "  'TN': 331,\n",
       "  'FP': 26,\n",
       "  'FN': 179,\n",
       "  'Precision': 0.9628040057224606,\n",
       "  'Recall': 0.789906103286385,\n",
       "  'F1 Score': 0.8678272082527402,\n",
       "  'Accuracy': 0.8304383788254756},\n",
       " 'Class acc': {'TP': 243,\n",
       "  'TN': 808,\n",
       "  'FP': 135,\n",
       "  'FN': 23,\n",
       "  'Precision': 0.6428571428571429,\n",
       "  'Recall': 0.9135338345864662,\n",
       "  'F1 Score': 0.7546583850931677,\n",
       "  'Accuracy': 0.869313482216708},\n",
       " 'Class vgood': {'TP': 35,\n",
       "  'TN': 1145,\n",
       "  'FP': 23,\n",
       "  'FN': 6,\n",
       "  'Precision': 0.603448275862069,\n",
       "  'Recall': 0.8536585365853658,\n",
       "  'F1 Score': 0.7070707070707071,\n",
       "  'Accuracy': 0.9760132340777502},\n",
       " 'Class good': {'TP': 44,\n",
       "  'TN': 1129,\n",
       "  'FP': 30,\n",
       "  'FN': 6,\n",
       "  'Precision': 0.5945945945945946,\n",
       "  'Recall': 0.88,\n",
       "  'F1 Score': 0.7096774193548386,\n",
       "  'Accuracy': 0.9702233250620348},\n",
       " 'Overall Accuracy': 0.9114971050454922}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(tree_RealData, df_train, \"class\", [\"unacc\", \"acc\", \"vgood\", \"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Class unacc': {'TP': 270,\n",
       "  'TN': 134,\n",
       "  'FP': 27,\n",
       "  'FN': 88,\n",
       "  'Precision': 0.9090909090909091,\n",
       "  'Recall': 0.7541899441340782,\n",
       "  'F1 Score': 0.8244274809160305,\n",
       "  'Accuracy': 0.7784200385356455},\n",
       " 'Class acc': {'TP': 92,\n",
       "  'TN': 331,\n",
       "  'FP': 70,\n",
       "  'FN': 26,\n",
       "  'Precision': 0.5679012345679012,\n",
       "  'Recall': 0.7796610169491526,\n",
       "  'F1 Score': 0.6571428571428571,\n",
       "  'Accuracy': 0.815028901734104},\n",
       " 'Class vgood': {'TP': 15,\n",
       "  'TN': 484,\n",
       "  'FP': 11,\n",
       "  'FN': 9,\n",
       "  'Precision': 0.5769230769230769,\n",
       "  'Recall': 0.625,\n",
       "  'F1 Score': 0.6,\n",
       "  'Accuracy': 0.9614643545279383},\n",
       " 'Class good': {'TP': 16,\n",
       "  'TN': 482,\n",
       "  'FP': 18,\n",
       "  'FN': 3,\n",
       "  'Precision': 0.47058823529411764,\n",
       "  'Recall': 0.8421052631578947,\n",
       "  'F1 Score': 0.6037735849056604,\n",
       "  'Accuracy': 0.9595375722543352},\n",
       " 'Overall Accuracy': 0.8786127167630058}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(tree_RealData, df_test, \"class\", [\"unacc\", \"acc\", \"vgood\", \"good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] *Decision Tree Classification in Python*, DataCamp, https://www.datacamp.com/tutorial/decision-tree-classification-python.\n",
    "\n",
    "[2] S. Aning and M. Przybyła-Kasperek, *Comparative Study of Twoing and Entropy Criterion for Decision*, in Proceedings of the 26th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2022), University of Silesia in Katowice, Institute of Computer Science, Be ̧dzin ́ska 39, 41-200 Sosnowiec, Poland.\n",
    "\n",
    "[3] *Decision Tree Classifier Tutorial*, Kaggle, https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial/notebook#8.-Import-dataset-."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
